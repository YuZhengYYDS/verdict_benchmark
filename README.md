# VERDICT Benchmark: Deep Learning for Medical Parameter Prediction

A comprehensive benchmark suite for evaluating deep learning models on VERDICT (Vascular, Extracellular, and Restricted Diffusion for Cytometry in Tumours) parameter prediction from medical imaging data.

## ğŸ”¬ What is VERDICT?

VERDICT is an advanced diffusion MRI technique that provides quantitative biomarkers for tissue microstructure analysis. It models tissue as three distinct compartments:
- **Vascular**: Blood vessels and vasculature
- **Extracellular**: Extracellular space
- **Restricted**: Intracellular space (cells)

This benchmark evaluates how well different neural network architectures can predict these critical medical parameters from imaging features.

## ğŸ¯ Project Overview

This benchmark provides:
- **State-of-the-art Models**: From simple MLPs to advanced architectures
- **Comprehensive Evaluation**: Statistical analysis with confidence intervals
- **Standardized Training**: Consistent protocols across all models
- **Publication-ready Results**: LaTeX tables and research-grade figures
- **Extensible Framework**: Easy to add new models and datasets

## ğŸ—ï¸ Architecture Zoo

Our benchmark includes diverse neural network architectures, each with detailed documentation:

### ğŸ“Š Feedforward Networks
- **[Multi-Layer Perceptron (MLP)](docs/models/README_MLP.md)** - Simple yet effective baseline
  - 3-layer architecture with configurable activations
  - Fast training and inference
  - Excellent starting point for tabular data

- **[Simple ResNet (Residual MLP)](docs/models/README_ResidualMLP.md)** - Enhanced with skip connections
  - Solves vanishing gradient problem
  - Enables deeper network training
  - Better performance on complex patterns

### ğŸŒŠ Sequence Models
- **[Recurrent Neural Network (RNN)](docs/models/README_RNN.md)** - Temporal pattern modeling
  - LSTM/GRU variants for sequence processing
  - Adaptive input reshaping strategies
  - Captures sequential dependencies

- **[Transformer](docs/models/README_Transformer.md)** - Attention-based architecture
  - Multi-head self-attention mechanisms
  - Parallel processing capabilities
  - Global feature relationship modeling

### ğŸ” Convolutional Networks
- **[Convolutional Neural Network (CNN)](docs/models/CNN_IMPROVEMENTS.md)** - Spatial pattern recognition
  - 1D convolutions for feature extraction
  - Hierarchical representation learning
  - Translation-invariant features

### ğŸ§  Advanced Architectures
- **[Variational Autoencoder (VAE)](docs/models/README_VAE.md)** - Probabilistic latent modeling
  - Learns compressed representations
  - Uncertainty quantification
  - Generative capabilities

- **[Mixture of Experts (MoE)](docs/models/MOE_ARCHITECTURE.md)** - Ensemble learning
  - Specialized expert networks
  - Dynamic routing mechanisms
  - Scalable model capacity

## ğŸš€ Quick Start

### Prerequisites
```bash
# Python 3.8+ required
pip install torch torchvision torchaudio
pip install -r requirements_eval.txt

# Optional: Install in development mode
pip install -e .
```

### Installation Verification
```bash
# Check if installation is working
python -c "from models.mlp import MLP; print('âœ… Models imported successfully')"
python -c "from data.dataset import VERDICTDataset; print('âœ… Dataset imported successfully')"
python -c "from utils.metrics import calculate_metrics; print('âœ… Utils imported successfully')"
```

### 1. Training Models
```bash
# Train individual models
python train.py --config configs/mlp.yaml
python train.py --config configs/transformer.yaml
python train.py --config configs/cnn_advanced.yaml

# Train all models (Windows)
run_evaluation.bat
```

### 2. Evaluation
```bash
# Basic evaluation
python evaluate_models.py --config configs/mlp.yaml

# Advanced statistical analysis
python advanced_evaluate.py --config configs/mlp.yaml

# Train and evaluate DenseNet model
python train.py --config configs/densenet_regressor.yaml

# Automated evaluation (recommended)
run_evaluation.bat
```

### 3. Results
Results are automatically saved to:
- `evaluation_results/` - Basic performance metrics
- `advanced_evaluation/` - Statistical analysis and publication-ready figures
- `wandb/` - Weights & Biases experiment tracking
- `checkpoints/` - Trained model weights and scalers

## ï¿½ Model Implementation Status

| Model | Implementation | Config | Documentation | Status |
|-------|---------------|---------|---------------|---------|
| MLP | âœ… `mlp.py` | âœ… `mlp.yaml` | âœ… `README_MLP.md` | Ready |
| Residual MLP | âœ… `residual_mlp.py` | âœ… `residual_mlp.yaml` | âœ… `README_ResidualMLP.md` | Ready |
| RNN/LSTM | âœ… `rnn.py` | âœ… `rnn.yaml` | âœ… `README_RNN.md` | Ready |
| Transformer | âœ… `transformer.py` | âœ… `transformer.yaml` | âœ… `README_Transformer.md` | Ready |
| CNN | âœ… `cnn.py` | âœ… `cnn_advanced.yaml` | âœ… `CNN_IMPROVEMENTS.md` | Ready |
| VAE | âœ… `vae_regressor.py` | âœ… `vae_regressor.yaml` | âœ… `README_VAE.md` | Ready |
| MoE | âœ… `moe_regressor.py` | âœ… `moe_regressor.yaml` | âœ… `MOE_ARCHITECTURE.md` | Ready |

*Note: TabNet implementation is referenced in performance tables but implementation files are not yet available in the repository.*

## ï¿½ğŸ“ˆ Performance Overview

| Model | RÂ² Score | RMSE | Training Time | Parameters |
|-------|----------|------|---------------|------------|
| MLP | 0.527 | 0.08-0.12 | 10-20 min | ~50K |
| Residual MLP | 0.532 | 0.07-0.11 | 15-25 min | ~60K |
| RNN (LSTM) | 0.480 | 0.08-0.13 | 20-35 min | ~80K |
| Transformer | 0.524 | 0.07-0.12 | 15-30 min | ~100K |
| CNN | 0.88-0.95 | 0.06-0.10 | 25-40 min | ~120K |
| VAE | 0.463 | 0.08-0.12 | 25-40 min | ~150K |
| MoE | 0.440 | 0.05-0.09 | 45-60 min | ~200K |

*Performance ranges reflect different hyperparameter configurations and dataset splits.*

## ğŸ“Š Comprehensive Evaluation

### Basic Metrics
- **RÂ² Score**: Coefficient of determination
- **RMSE**: Root Mean Square Error
- **MAE**: Mean Absolute Error
- **Per-parameter Analysis**: Individual parameter performance

### Advanced Statistics
- **Statistical Significance**: Pairwise model comparisons
- **Bootstrap Confidence Intervals**: Uncertainty quantification
- **Effect Sizes**: Practical significance assessment

### Evaluation Documentation
- **[Evaluation Guide](docs/eval/EVALUATION_README.md)** - Complete evaluation instructions

## ğŸ› ï¸ Project Structure

```
verdict_benchmark/
â”œâ”€â”€ ğŸ“ models/                   # Model implementations
â”‚   â”œâ”€â”€ mlp.py                   # Multi-Layer Perceptron
â”‚   â”œâ”€â”€ residual_mlp.py          # Residual MLP
â”‚   â”œâ”€â”€ rnn.py                   # RNN/LSTM/GRU
â”‚   â”œâ”€â”€ transformer.py           # Transformer
â”‚   â”œâ”€â”€ cnn.py                   # Convolutional Network
â”‚   â”œâ”€â”€ densenet_regressor.py    # DenseNet Regressor
â”‚   â”œâ”€â”€ vae_regressor.py         # Variational Autoencoder
â”‚   â””â”€â”€ moe_regressor.py         # Mixture of Experts
â”œâ”€â”€ ğŸ“ configs/                  # Configuration files
â”‚   â”œâ”€â”€ mlp.yaml                 # MLP settings
â”‚   â”œâ”€â”€ transformer.yaml         # Transformer settings
â”‚   â”œâ”€â”€ cnn_advanced.yaml        # CNN settings
â”‚   â”œâ”€â”€ rnn.yaml                 # RNN settings
â”‚   â”œâ”€â”€ residual_mlp.yaml        # Residual MLP settings
â”‚   â”œâ”€â”€ densenet_regressor.yaml  # DenseNet settings
â”‚   â”œâ”€â”€ vae_regressor.yaml       # VAE settings
â”‚   â””â”€â”€ moe_regressor.yaml       # MoE settings
â”œâ”€â”€ ğŸ“ docs/                     # Documentation
â”‚   â”œâ”€â”€ models/                  # Model documentation
â”‚   â”‚   â”œâ”€â”€ README_MLP.md        # MLP guide
â”‚   â”‚   â”œâ”€â”€ README_ResidualMLP.md # Residual MLP guide
â”‚   â”‚   â”œâ”€â”€ README_RNN.md        # RNN guide
â”‚   â”‚   â”œâ”€â”€ README_Transformer.md # Transformer guide
â”‚   â”‚   â”œâ”€â”€ README_DenseNet.md   # DenseNet guide
â”‚   â”‚   â”œâ”€â”€ README_VAE.md        # VAE guide
â”‚   â”‚   â”œâ”€â”€ CNN_IMPROVEMENTS.md  # CNN enhancements
â”‚   â”‚   â””â”€â”€ MOE_ARCHITECTURE.md  # MoE architecture
â”‚   â””â”€â”€ eval/                    # Evaluation documentation
â”‚       â””â”€â”€ EVALUATION_README.md # Evaluation guide
â”œâ”€â”€ ğŸ“ data/                     # Dataset utilities
â”‚   â”œâ”€â”€ dataset.py               # Data loading
â”‚   â””â”€â”€ demodataset.ipynb        # Data exploration
â”œâ”€â”€ ğŸ“ utils/                    # Utility functions
â”‚   â”œâ”€â”€ metrics.py               # Evaluation metrics
â”‚   â””â”€â”€ scaler.py                # Data preprocessing
â”œâ”€â”€ ğŸ“ checkpoints/              # Trained models
â”œâ”€â”€ ğŸ“ logs/                     # Training logs
â”œâ”€â”€ ğŸ“„ train.py                  # Training script
â”œâ”€â”€ ğŸ“„ evaluate_models.py        # Basic evaluation
â”œâ”€â”€ ğŸ“„ advanced_evaluate.py      # Advanced analysis
â”œâ”€â”€ ğŸ“„ run_evaluation.bat        # Automated evaluation
â””â”€â”€ ğŸ“„ setup.py                  # Package setup
```

## ğŸ“ Research Applications

### Medical Imaging
- **Cancer Research**: Tumor microenvironment analysis
- **Treatment Monitoring**: Therapy response assessment
- **Diagnostic Support**: Quantitative biomarker extraction

### Machine Learning
- **Architecture Comparison**: Systematic model evaluation
- **Tabular Learning**: Benchmark for structured data
- **Medical AI**: Healthcare-specific deep learning

### Publications
This benchmark has been designed to support:
- **Reproducible Research**: Standardized evaluation protocols
- **Fair Comparison**: Consistent training and evaluation
- **Statistical Rigor**: Proper significance testing
- **Publication Quality**: LaTeX tables and figures

## ğŸ”¬ Dataset Information

### VERDICT Training Data
- **Features**: 153-dimensional imaging features
- **Targets**: 3 VERDICT parameters (vascular, extracellular, restricted)
- **Samples**: Professional medical imaging dataset
- **Preprocessing**: Standardized scaling and normalization

### Data Loading
```python
from data.dataset import VERDICTDataset
dataset = VERDICTDataset(mat_path="path/to/TrainingSet.mat")
```

## ğŸ“š Model Documentation

Each model includes comprehensive documentation:

### Architecture Guides
- **[MLP README](docs/models/README_MLP.md)** - Simple feedforward networks
- **[Residual MLP README](docs/models/README_ResidualMLP.md)** - Skip connections and deep networks
- **[RNN README](docs/models/README_RNN.md)** - Sequence modeling with LSTM/GRU
- **[Transformer README](docs/models/README_Transformer.md)** - Attention mechanisms
- **[DenseNet README](docs/models/README_DenseNet.md)** - Dense connections and feature reuse
- **[VAE README](docs/models/README_VAE.md)** - Variational autoencoders
- **[MOE README](docs/models/MOE_ARCHITECTURE.md)** - Ensemble learning with specialized expert networks

## ğŸ¯ Customization

### Adding New Models
1. Create model class in `models/`
2. Add configuration in `configs/`
3. Update training script imports
4. Create model-specific README

### Custom Datasets
1. Implement dataset class in `data/`
2. Update configuration files
3. Adjust input/output dimensions
4. Modify evaluation metrics if needed

### Hyperparameter Tuning
Each model includes extensive hyperparameter documentation:
- Architecture scaling guidelines
- Training parameter suggestions
- Regularization techniques
- Performance optimization tips

## ğŸ” Advanced Features

### Weights & Biases Integration
```yaml
wandb_project: verdict_benchmark
wandb_run_name: model_experiment
```

### Learning Rate Scheduling
```yaml
scheduler:
  type: CosineAnnealingWarmRestarts
  T_0: 15
  T_mult: 2
  eta_min: 0.000001
```

### Early Stopping
```yaml
early_stop_patience: 40
```

## ğŸ¤ Contributing

We welcome contributions! Please see our contribution guidelines:

1. **Fork** the repository
2. **Create** a feature branch
3. **Add** your model or improvement
4. **Test** thoroughly
5. **Submit** a pull request

### Areas for Contribution
- New model architectures
- Evaluation metrics
- Visualization improvements
- Documentation enhancements
- Performance optimizations

## ğŸ“„ Citation

If you use this benchmark in your research, please cite:

```bibtex
@misc{verdict_benchmark2025,
  title={VERDICT Benchmark: Deep Learning for Medical Parameter Prediction},
  author={Zheng Yu, Matteo Figini, ...},
  year={2025},
  month={July},
  url={NA},
  note={A comprehensive benchmark suite for evaluating deep learning models on VERDICT parameter prediction}
}
```

## ğŸ”— Related Work

- **VERDICT MRI**: Original diffusion MRI technique for tissue microstructure analysis
- **Medical AI Benchmarks**: Related benchmarks in medical imaging and deep learning
- **Tabular Learning**: Advances in neural networks for structured data
- **PyTorch Ecosystem**: Deep learning frameworks and tools

## ğŸ“ Support

### Troubleshooting

**Common Issues:**
- **ImportError**: Make sure all dependencies are installed with `pip install -r requirements_eval.txt`
- **CUDA Issues**: Ensure PyTorch is installed with CUDA support if using GPU
- **Memory Errors**: Reduce batch size in config files for large models
- **Config Errors**: Check YAML syntax and ensure all required fields are present

**Performance Tips:**
- Use GPU for faster training (CUDA compatible)
- Adjust batch size based on available memory
- Enable mixed precision training for memory efficiency
- Use early stopping to prevent overfitting

### Documentation
- **[Evaluation Guide](docs/eval/EVALUATION_README.md)** - Complete evaluation instructions
- **[Model READMEs](docs/models/)** - Individual architecture documentation
- **[CNN Improvements](docs/models/CNN_IMPROVEMENTS.md)** - CNN-specific enhancements
- **[MoE Architecture](docs/models/MOE_ARCHITECTURE.md)** - Mixture of Experts details

### Issues
- Check existing issues on GitHub
- Create detailed bug reports
- Include configuration files and logs
- Provide minimal reproducible examples

## ğŸ“œ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- VERDICT methodology developers
- Medical imaging research community
- PyTorch and scientific computing ecosystem
- Open source contributors

